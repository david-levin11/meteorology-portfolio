{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david-levin11/meteorology-portfolio/blob/main/NBM_Percentile_in_Context_EXP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj77e3ZfajKO"
      },
      "source": [
        "**NBM Percentile in Context**\n",
        "A script to grab NBM percentile data, deterministic forecast and obs, and put the obs or deterministic forecast into the context of Probabilistic NBM space. This is based on code originally developed by Caleb Steele of Western Region.\n",
        "\n",
        "-*Steve Levine - NWS MDL/Statistical Modeling Division - 2-March-2023*<br/>\n",
        "-*David Levin - NWS Alaska Region ESSD - 5-July-2024*\n",
        "<br/> <br/>\n",
        "Updates:\n",
        "> ***BUG:  StageIV QPE data no longer works due to new NWPS...need to reference IDP GIS***\n",
        "\n",
        "> 15-Jul-2024 Fixed bug which was not displaying map for all Alaska Region\n",
        "\n",
        ">15-Jul-2024 Included functionality to look at observations within a custom bounding box\n",
        "\n",
        "> 5-Jul-2024 Included OCONUS functionality (AR and HFO and Puerto Rico) and also the ability to use the 24hr APRFC QC'ed gauge data as a verification source for PQPF 24hr products\n",
        "\n",
        "> 20-Sept-2023: Included ability to work with HI/PR/AK domains (still no GU yet)\n",
        "\n",
        ">14-Sep-2023: Updated snow plotting to work with 'unknown' name in snow total grib2 message\n",
        "\n",
        ">10-Jul-2023: Corrected smoothling spline of percentiles so that now spline passes through relevant end points (set s = 0)\n",
        "\n",
        "> 20-June 2023: Added deterministic 24-hour snow forecasts, which include same caveats as 24-hour QPF forecasts (random bin assignment, etc.)\n",
        "\n",
        "> 4-Apr-2023: Added probabilistic (but not determinisitc) 24-hour snow forecasts based on NOHRSC analysis, added zero-padding for time strings where needed.  Set observed or deterministic qpf/snow to zero when value is < 0.005 inches.  Also added minor edits to plots (CWA's are now outlined in black).\n",
        "\n",
        "> 30-Mar-2023: Fixed bug where 0.0 qpf obs were being set to NaN.  Also, assigned such obs to a random percentile bin where percentile values was also 0.0.\n",
        "\n",
        "> 13-Mar-2023: Fixed bug with random extra obs showing up.  Also inserted exception for maximum winds, which do not exist in core/deterministic.  Fatal error will generate when attempting to work with core/deterministic max wind.\n",
        "\n",
        "> 2-Mar-2023: Added capability for maximum wind forecast.  Note that a determinsitic/core maximum wind forecast is not available.\n",
        "\n",
        ">>Previous development was from Caleb Steele\n",
        "\n",
        "> 12-Aug-2022: Pretty significant update that changed much of the underlying code. Files used are now grib files (instead of the geotiffs used before), which are larger and take longer to download and decode. The gribs include more percentiles, and a cubic spline is utilized vs linear interpolation now, so the NBM distribution is much better sampled. While all 99 percentiles are avaiable, trimmed it to use 13 (1st, 5th, 10th, 20th, 30th, etc.) to save some time. If you are more patient, you can dig into the code and swap out the lists (uncomment one, uncomment the other) that will use all 99, but it will take awhile. In my limited tests, it offers little improvement in the representation of the CDF/PDF. Also wrapped everything up behind a form and added light/dark mode option. Finally, added an option to generate a csv from the dataframe that is created (helpful to make sure it has done what you expect).\n",
        "\n",
        "> 7-Apr-2022: Added QPF, but still a lot of cleanup required. If you select Deterministic and QPF, it will really use the percentile mean. Will clean it up to use the deterministic, and add percentile mean as a separate option at some point.\n",
        "\n",
        "> 18-Feb-2022: Added option of adding CWA boundaries, cleaned up the directories (by actually making some), and more plot tweaks so they all look as expected.\n",
        "\n",
        "> 17-Feb-2022: fixed a hard coded reference that lead to the histogram always displaying the observation percentile distribution, even when deterministic was selected in regional plots.\n",
        "\n",
        "> 16-Feb-2022: added \"compare_to\" variable which lets you switch between comparing obs and NBM determinsitic to the ProbMaxT Percentiles.\n",
        "\n",
        "> 14-Jul-2025:  Added the ability to work with NBM 5.0 data and added MaxWind for Alaska\n",
        "\n",
        "> 26-Aug-2025:  Updated import procedure to remove dependency on conda and to run with the latest update to colab to python 3.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7inkOnFayzV"
      },
      "source": [
        "This first two cells just import everything we need. **You'll only need to run steps 1 and 2 one time.  After that you can repeat steps 3 and after as many times as you need!**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.  Install required packages with pip\n",
        "!pip install -q numpy pandas scipy matplotlib seaborn contextily pyproj pygrib netCDF4 cartopy shapely"
      ],
      "metadata": {
        "id": "SAfL8UId5pUY",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Import packages\n",
        "import numpy as np\n",
        "from scipy.interpolate import CubicSpline as cs, UnivariateSpline as us\n",
        "import pandas as pd\n",
        "from urllib.request import urlretrieve, urlopen\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "from netCDF4 import Dataset\n",
        "import pygrib\n",
        "import pyproj\n",
        "from pyproj import Proj, transform\n",
        "import os, re, traceback\n",
        "import sys\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "#from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.axes as maxes\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from matplotlib.path import Path\n",
        "from matplotlib.textpath import TextToPath\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.font_manager import FontProperties\n",
        "matplotlib.rcParams['font.sans-serif'] = 'Liberation Sans'\n",
        "matplotlib.rcParams['font.family'] = \"sans-serif\"\n",
        "from matplotlib.cm import get_cmap\n",
        "import seaborn as sns\n",
        "\n",
        "from cartopy import crs as ccrs, feature as cfeature\n",
        "from cartopy.io.shapereader import Reader\n",
        "import cartopy.io.shapereader as shpreader\n",
        "from cartopy.feature import ShapelyFeature\n",
        "import contextily as cx\n",
        "import itertools\n",
        "\n",
        "import zipfile\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "bnz_Ok9iyVbS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second cell should be run right after the first cell.  Note that these first two cells only need to be run once; then they will work for all other cases until you close or reset this notebook."
      ],
      "metadata": {
        "id": "wskTjNF1tCqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLyMTMsby8d8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title 3. Select Options and Download Obs { display-mode: \"form\" }\n",
        "#@markdown If you are using raw obs, you'll need to enter your token for the\n",
        "#@markdown Synoptic API below.  Below are instructions on how to get a token:\n",
        "#@markdown https://docs.google.com/document/d/1YuMUYog4J7DpFoEszMmFir4Ehqk9Q0GHG_QhSdrgV9M/edit?usp=sharing\n",
        "synoptic_token = \"\" #@param {type:\"string\"}\n",
        "#@markdown Which probabilistic element from the NBM?  **Note that probabilistic winds are only available for AK if you select NBM 5.0 below.**\n",
        "element = \"maxt\" #@param [\"maxt\", \"mint\",\"qpf\",\"maxwind\",\"snow\"]\n",
        "valid_date = \"2025-08-30\" #@param {type:\"date\"}\n",
        "#@markdown QPF valid 24 hours ending time\n",
        "qpf_valid_time = 12 #@param {type:\"slider\", min:0, max:18, step:6}\n",
        "#@markdown APRFC QC'ed gauge data instead of obs (can only check one)?\n",
        "\n",
        "#@markdown QC gauge data is only for 24hrs valid at 12z of a calendar day.\n",
        "\n",
        "#@markdown NOHRSC data is only available for CONUS.\n",
        "\n",
        "#@markdown **Leaving both of these boxes unchecked is the default and results in using raw obs.**\n",
        "#use_stageiv = True #@param {type:\"boolean\"}\n",
        "# Stage IV needs to be fixed due to new NWPS page\n",
        "# need to use map server at: https://mapservices.weather.noaa.gov/raster/rest/services/obs/rfc_qpe/MapServer\n",
        "\n",
        "use_stageiv = False\n",
        "use_nohrsc = False #@param {type:\"boolean\"}\n",
        "use_QC_Gauge_Data = False #@param {type:\"boolean\"}\n",
        "#@markdown **Which NBM version would you like to plot (the current operational version is 4.3)**\n",
        "nbm_version = \"4.3\" #@param [\"4.3\", \"5.0\"]\n",
        "#@markdown Pick NBM run time (note: add 1 hour for snow fcsts)\n",
        "nbm_init_date = \"2025-08-29\" #@param {type:\"date\"}\n",
        "nbm_init_hour = 0 #@param {type:\"slider\", min:0, max:18, step:6}\n",
        "#@markdown Where do you want to focus?\n",
        "region_selection = \"CWA\" #@param [\"WR\", \"SR\", \"CR\", \"ER\", \"AR\", \"CONUS\", \"CWA\"]\n",
        "#@markdown If CWA selected, which one? (i.e. \"SLC\" for Salt Lake City)\n",
        "cwa_id = \"AJK\" #@param {type:\"string\"}\n",
        "compare_to = \"obs\" #@param [\"obs\", \"deterministic\"]\n",
        "#@markdown Which obs?\n",
        "network_selection = \"ALL\" #@param [\"NWS\", \"RAWS\", \"NWS+RAWS\", \"NWS+RAWS+HADS\", \"ALL\", \"CUSTOM\", \"LIST\"]\n",
        "#@markdown If Custom or List selected for network, enter comma separated network IDs (custom) or siteids (list)  WITH NO SPACES here. For help - https://developers.synopticdata.com/about/station-providers/\n",
        "network_input = \"\"#@param {type:\"string\"}\n",
        "#@markdown Elevation filter?\n",
        "elev_filter = False #@param {type:\"boolean\"}\n",
        "#@markdown If elevation filter is checked which elevation to you want to filter (ft)?\n",
        "elev_filter_value = 25 #@param {type:\"number\"}\n",
        "#@markdown Would you like to filter out obs above or below this elevation?\n",
        "filter_above_below = \"above\" #@param [\"above\", \"below\"]\n",
        "#@markdown Plot CWA boundaries?\n",
        "cwa_outline = False #@param {type:\"boolean\"}\n",
        "county_outline = False #@param {type:\"boolean\"}\n",
        "#@markdown Do you want a CSV?\n",
        "export_csv = True #@param {type:\"boolean\"}\n",
        "#@markdown Light or dark theme plots?\n",
        "plot_style = \"dark\" #@param [\"light\", \"dark\"]\n",
        "#@markdown Custom zoom area for plot (if yes then enter comma seperated lat/lon pairs for your bounding box below (Ex:  custom_southwest = 62.45,-155.55)?\n",
        "custom_area = False #@param {type:\"boolean\"}\n",
        "#@markdown What would you like to name your custom area (mainly for file saving purposes)\n",
        "custom_area_name = \"AncBowl\" #@param {type:\"string\"}\n",
        "custom_southwest = \"60.968512, -150.654583\" #@param {type: \"string\"}\n",
        "custom_northeast = \"61.819846, -148.602741\" #@param {type: \"string\"}\n",
        "#@markdown Plot cities on map?\n",
        "plot_cities = True #@param {type:\"boolean\"}\n",
        "#@markdown What population threshold would you like to use to thin out cities?\n",
        "pop_thresh = 1000 #@param\n",
        "if region_selection == \"CONUS\":\n",
        "  region_list = [\"WR\", \"CR\", \"SR\", \"ER\"]\n",
        "elif region_selection == \"CWA\":\n",
        "  region_list = [cwa_id]\n",
        "#elif region_selection == \"AR\":\n",
        "  #region_list=[\"AJK\",\"ARH\",\"AFC\"]\n",
        "else:\n",
        "  region_list = [region_selection]\n",
        "\n",
        "\n",
        "def cwa_list(input_region):\n",
        "  region_dict ={\"WR\":\"BYZ,BOI,LKN,EKA,FGZ,GGW,TFX,VEF,LOX,MFR,MTR,MSO,PDT,PSR,PIH,PQR,REV,STO,SLC,SGX,HNX,SEW,OTX,TWC\",\n",
        "              \"CR\":\"ABR,BIS,CYS,LOT,DVN,BOU,DMX,DTX,DDC,DLH,FGF,GLD,GJT,GRR,GRB,GID,IND,JKL,EAX,ARX,ILX,LMK,MQT,MKX,MPX,LBF,APX,IWX,OAX,PAH,PUB,UNR,RIW,FSD,SGF,LSX,TOP,ICT\",\n",
        "              \"ER\":\"ALY,LWX,BGM,BOX,BUF,BTV,CAR,CTP,RLX,CHS,ILN,CLE,CAE,GSP,MHX,OKX,PHI,PBZ,GYX,RAH,RNK,AKQ,ILM\",\n",
        "              \"SR\":\"ABQ,AMA,FFC,EWX,BMX,BRO,CRP,EPZ,FWD,HGX,HUN,JAN,JAX,KEY,MRX,LCH,LZK,LUB,MLB,MEG,MFL,MOB,MAF,OHX,LIX,OUN,SJT,SHV,TAE,TBW,TSA\",\n",
        "              \"AR\":\"AJK,AFG,AFC\"}\n",
        "  if (input_region in [\"WR\", \"CR\", \"SR\", \"ER\", \"AR\"]):\n",
        "    cwas_list = region_dict[input_region]\n",
        "  else:\n",
        "    cwas_list = input_region\n",
        "  return cwas_list\n",
        "\n",
        "def plot_towns(ax, south, north, west, east, population=5000, resolution='10m', transform=ccrs.PlateCarree(), zorder=3):\n",
        "    \"\"\"\n",
        "    This function will download the 'populated_places' shapefile from\n",
        "    NaturalEarth, trim the shapefile based on the limits of the provided\n",
        "    lat & long coords, and then plot the locations and names of the towns\n",
        "    on a given GeoAxes.\n",
        "\n",
        "    ax = a pyplot axes object\n",
        "    south = south lat limit (float)\n",
        "    north = north lat limit (float)\n",
        "    west = west long limit (float)\n",
        "    east = east long limit (float)\n",
        "    resolution= str. either high res:'10m' or low res: '50m'\n",
        "    population = minimum population of towns to plot (int)\n",
        "    transform = a cartopy crs object\n",
        "    \"\"\"\n",
        "    #get town locations\n",
        "    shp_fn = shpreader.natural_earth(resolution=resolution, category='cultural', name='populated_places')\n",
        "    shp = Reader(shp_fn)\n",
        "    xy = [pt.coords[0] for pt in shp.geometries()]\n",
        "    x, y = list(zip(*xy))\n",
        "\n",
        "    #get town names\n",
        "    towns = shp.records()\n",
        "    names_en = []\n",
        "    max_population = []\n",
        "    for town in towns:\n",
        "        #print(town.attributes)\n",
        "        names = town.attributes['NAME']\n",
        "        pop = town.attributes['POP_MAX']\n",
        "        names_en.append(names)\n",
        "        max_population.append(pop)\n",
        "    #print(names_en)\n",
        "    #create data frame and index by the region of the plot\n",
        "    all_towns = pd.DataFrame({'names_en': names_en, 'x':x, 'y':y, 'population':max_population})\n",
        "    #print(all_towns.head())\n",
        "    region_towns = all_towns[(all_towns.y<north) & (all_towns.y>south)\n",
        "                           & (all_towns.x>west) & (all_towns.x<east)]\n",
        "    region_towns = region_towns[region_towns.population > population]\n",
        "    #print(region_towns.head())\n",
        "    #plot the locations and labels of the towns in the region\n",
        "    ax.scatter(region_towns.x.values, region_towns.y.values, c ='white', marker= '.', transform=transform, zorder=zorder)\n",
        "    transform_mpl = ccrs.PlateCarree()._as_mpl_transform(ax) #this is a work-around to transform xy coords in ax.annotate\n",
        "    for i, txt in enumerate(region_towns.names_en):\n",
        "         ax.annotate(txt, (region_towns.x.values[i], region_towns.y.values[i]), xycoords=transform_mpl, color='white')\n",
        "\n",
        "nbm_init = datetime.strptime(nbm_init_date,'%Y-%m-%d') + timedelta(hours=int(nbm_init_hour))\n",
        "\n",
        "if element == \"maxt\":\n",
        "    nbm_core_valid_hour=\"00\"\n",
        "    nbm_qmd_valid_hour=\"06\"\n",
        "    valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(days=1)\n",
        "    obs_start_hour = \"1200\"\n",
        "    obs_end_hour = \"0600\"\n",
        "    ob_stat = \"maximum\"\n",
        "    valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    nbm_core_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_core_valid_hour))\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    core_init = nbm_init + timedelta(hours = 7)\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "\n",
        "elif element == \"mint\":\n",
        "    nbm_core_valid_hour=\"12\"\n",
        "    nbm_qmd_valid_hour=\"18\"\n",
        "    valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    obs_start_hour = \"0000\"\n",
        "    obs_end_hour = \"1800\"\n",
        "    ob_stat = \"minimum\"\n",
        "    valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    nbm_core_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_core_valid_hour))\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    core_init = nbm_init + timedelta(hours = 7)\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "\n",
        "elif element == \"qpf\":\n",
        "    nbm_core_valid_hour = (str(qpf_valid_time)).zfill(2)\n",
        "    nbm_valid_hour = (str(qpf_valid_time)).zfill(2)\n",
        "    nbm_qmd_valid_hour=(str(qpf_valid_time)).zfill(2)\n",
        "    valid_date = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(hours=int(qpf_valid_time))\n",
        "    valid_date_start = valid_date - timedelta(hours=24)\n",
        "    valid_date_end = valid_date\n",
        "    obs_start_hour = (str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    obs_end_hour = (str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    ob_stat = \"total\"\n",
        "    valid_end_datetime = valid_date_end\n",
        "    core_init = nbm_init\n",
        "    nbm_core_valid_end_datetime = valid_date_end\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - nbm_init\n",
        "\n",
        "elif element == \"maxwind\":\n",
        "    #nbm_core_valid_hour=\"06\"\n",
        "    #nbm_valid_hour=\"06\"\n",
        "    nbm_qmd_valid_hour=\"06\"\n",
        "    obs_start_hour=\"0600\"\n",
        "    obs_end_hour=\"0600\"\n",
        "    ob_stat=\"maximum\"\n",
        "    valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(days=1)\n",
        "    valid_end_datetime=valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    core_init = nbm_init\n",
        "    nbm_core_valid_end_datetime = valid_date_end\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    nbm_core_fhdelta = valid_end_datetime - nbm_init\n",
        "    if compare_to == \"deterministic\":\n",
        "      raise Exception(\"FATAL ERROR: You must compare to obs when looking at MAXWIND.  Deterministic data are not available!\")\n",
        "    #valid_date=date.strptime(valid_date,'') + timedelta(hours=)\n",
        "\n",
        "elif element == \"snow\":# or element == \"ice\":\n",
        "    nbm_core_valid_hour=(str(qpf_valid_time)).zfill(2)\n",
        "    nbm_qmd_valid_hour=(str(qpf_valid_time)).zfill(2)\n",
        "    obs_start_hour=(str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    obs_end_hour = (str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    valid_date = datetime.strptime(valid_date,'%Y-%m-%d') #+ timedelta(hours=int(qpf_valid_time))\n",
        "    valid_date_start = valid_date - timedelta(hours=24)\n",
        "    valid_date_end = valid_date\n",
        "    valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    ob_stat = \"total\"\n",
        "    core_init = nbm_init + timedelta(hours = 1)\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    nbm_core_valid_end_datetime = nbm_qmd_valid_end_datetime\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "    #if compare_to == \"deterministic\":\n",
        "    #  raise Exception(\"FATAL ERROR: You must compare to obs when looking at snow/ice.  Determinsitic data are not avialable yet.\")\n",
        "\n",
        "current_datetime = datetime.now()\n",
        "\n",
        "nbm_core_forecasthour = nbm_core_fhdelta.total_seconds() / 3600.\n",
        "if element == \"snow\" or element == \"qpf\":\n",
        "  nbm_core_forecasthour_start = nbm_core_forecasthour - 24\n",
        "else:\n",
        "  nbm_core_forecasthour_start = nbm_core_forecasthour - 12\n",
        "nbm_qmd_fhdelta = nbm_qmd_valid_end_datetime - nbm_init\n",
        "nbm_qmd_forecasthour = nbm_qmd_fhdelta.total_seconds() / 3600.\n",
        "if element == \"qpf\" or element == \"maxwind\" or element == \"maxgust\" or element == \"snow\" or element == \"ice24\":\n",
        "  nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 24\n",
        "else:\n",
        "  nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 18\n",
        "\n",
        "# checking to make sure our qpf data selection is valid\n",
        "if element == 'qpf' and use_QC_Gauge_Data:\n",
        "  today = datetime.utcnow()\n",
        "  print(today)\n",
        "  qpeday = ((today-valid_date_end)+timedelta(days=1)).days\n",
        "  print('QPE day is day '+str(qpeday))\n",
        "  if qpeday > 29:\n",
        "    print(\"Valid date is outside the range of RFC QPE files...switching to raw obs\")\n",
        "    use_QC_Gauge_Data = False\n",
        "  if qpf_valid_time != 12:\n",
        "    print(\"You have selected a QPF valid time of \"+ str(qpf_valid_time))\n",
        "    print(\"This is invalid as QC gauge data is only valid at 12z...switching to raw obs!\")\n",
        "    use_QC_Gauge_Data = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "statistics_api = \"https://api.synopticlabs.org/v2/stations/legacystats?\"\n",
        "precipitation_api = \"https://api.synopticdata.com/v2/stations/precipitation?\"\n",
        "metadata_api = \"https://api.synopticdata.com/v2/stations/metadata?\"\n",
        "\n",
        "# Setup a diting a form selection into a sometctionary for translahing we can pass to mesowest API\n",
        "network_dict = {\"NWS+RAWS+HADS\":\"&network=1,2,106\",\"NWS+RAWS\":\"&network=1,2\", \"NWS\":\"&network=1\", \"RAWS\": \"&network=2\", \"ALL\":\"\", \"CUSTOM\": \"&network=\"+network_input, \"LIST\": \"&stid=\"+network_input}\n",
        "network_string = network_dict[network_selection]\n",
        "\n",
        "if element == \"qpf\":\n",
        "  cmap = get_cmap('PiYG')\n",
        "  cmap.set_under(color='red')\n",
        "  cmap.set_over(color='yellow')\n",
        "elif element == \"snow\":\n",
        "  cmap = get_cmap('cool_r')\n",
        "  cmap.set_under(color='black')\n",
        "  cmap.set_over(color='yellow')\n",
        "else:\n",
        "  #cmap = 'Spectral'\n",
        "  cmap = get_cmap('bwr')\n",
        "  cmap.set_under(color='yellow')\n",
        "  cmap.set_over(color='black')\n",
        "if use_stageiv and element==\"qpf\":\n",
        "  points_str = f'Stage IV @ {network_selection}'\n",
        "else:\n",
        "  points_str = network_selection\n",
        "\n",
        "if plot_style==\"light\":\n",
        "  background_color = '#f7f7f7'\n",
        "  text_color = '#121212'\n",
        "  map_land_color = '#FAFAF8'\n",
        "  map_water_color = '#D4DBDD'\n",
        "  map_border_color = 'grey'\n",
        "elif plot_style==\"dark\":\n",
        "  background_color = '#272727'\n",
        "  text_color = 'white'\n",
        "  map_land_color = '#414143'\n",
        "  map_water_color = '#272727'\n",
        "  #map_border_color = '#3B3B3D'\n",
        "  map_border_color = 'white'\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "# Reusable functions section                                                                                           #\n",
        "########################################################################################################################\n",
        "\n",
        "def project3(lon, lat, prj):\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "\n",
        "  outproj = prj\n",
        "  inproj = Proj(init='epsg:4326')\n",
        "  nbm_coords = transform(inproj, outproj, lon, lat)\n",
        "  coordX = nbm_coords[0]\n",
        "  coordY = nbm_coords[1]\n",
        "  #print(f'Lat: {lat}, Y: {coordY} | Lon: {lon}, X: {coordX}')\n",
        "  return(coordX, coordY)\n",
        "\n",
        "\n",
        "def ll_to_index(datalons, datalats, loclon, loclat):\n",
        "  abslat = np.abs(datalats-loclat)\n",
        "  abslon = np.abs(datalons-loclon)\n",
        "  c = np.maximum(abslon, abslat)\n",
        "  latlon_idx_flat = np.argmin(c)\n",
        "  latlon_idx = np.unravel_index(latlon_idx_flat, datalons.shape)\n",
        "  return(latlon_idx)\n",
        "\n",
        "\n",
        "def project_hrap(lon, lat, s4x, s4y):\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "\n",
        "  globe = ccrs.Globe(semimajor_axis=6371200)\n",
        "  hrap_ccrs = proj = ccrs.Stereographic(central_latitude=90.0,\n",
        "                          central_longitude=255.0,\n",
        "                          true_scale_latitude=60.0, globe=globe)\n",
        "  latlon_ccrs = ccrs.PlateCarree()\n",
        "  hrap_coords = hrap_ccrs.transform_point(lon,lat,src_crs=latlon_ccrs)\n",
        "  hrap_idx = ll_to_index(s4x, s4y, hrap_coords[0], hrap_coords[1])\n",
        "\n",
        "  return hrap_idx\n",
        "\n",
        "def nohrsc_ll2ij(lon,lat,gridlons,gridlats):\n",
        "  #for a lat/lon grid\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "  lonidx=(np.abs(lon-gridlons)).argmin()\n",
        "  latidx=(np.abs(lat-gridlats)).argmin()\n",
        "  return(latidx,lonidx)\n",
        "\n",
        "def get_stageiv():\n",
        "  siv_url = \"https://water.weather.gov/precip/downloads/\"+valid_date_end.strftime('%Y')+\"/\"+valid_date_end.strftime('%m')+\"/\"+valid_date_end.strftime('%d')+\"/nws_precip_1day_\"+valid_date_end.strftime('%Y%m%d')+\"_conus.nc\"\n",
        "  data = urlopen(siv_url).read()\n",
        "  print(siv_url)\n",
        "  print(data)\n",
        "  print(f'Valid date end is: {valid_date_end}')\n",
        "  nc = Dataset('data', memory=data)\n",
        "  #with Dataset(siv_file, 'r') as nc:\n",
        "  stageIV = nc.variables['observation']\n",
        "  s4x = nc.variables['x']\n",
        "  s4y = nc.variables['y']\n",
        "  return stageIV, s4x, s4y\n",
        "\n",
        "def get_nohrsc():\n",
        "  nohrsc_url = \"https://www.nohrsc.noaa.gov/snowfall_v2/data/\"+valid_date_end.strftime('%Y%m')+\"/sfav2_CONUS_24h_\"+valid_date_end.strftime('%Y%m%d%H')+\".nc\"\n",
        "  data = urlopen(nohrsc_url).read()\n",
        "\n",
        "  nc = Dataset('data',memory=data)\n",
        "  snow=np.asarray(nc.variables['Data']) #make lon by lat array (original lat by lon)\n",
        "  snowlat = np.asarray(nc.variables['lat'])\n",
        "  snowlon = np.asarray(nc.variables['lon'])\n",
        "  return snow,snowlon,snowlat\n",
        "\n",
        "def get_qc_data():\n",
        "    today = datetime.utcnow()\n",
        "    print(today)\n",
        "    qpeday = (today-valid_date_end)+timedelta(days=1)\n",
        "    print(qpeday.days)\n",
        "    try:\n",
        "      qcurl = f'https://www.weather.gov/source/aprfc/verification/QPEday{qpeday.days}.csv'\n",
        "      csv_name = f'{qpeday.days}.csv'\n",
        "      print(f'QC url is: {qcurl}')\n",
        "      response = requests.get(qcurl)\n",
        "      with open(csv_name, 'wb') as f:\n",
        "          f.write(response.content)\n",
        "      with open(csv_name, 'r') as fl:\n",
        "          header = fl.readline().strip()\n",
        "          fl_date_raw = header.split(' - ')[1]\n",
        "          fl_date = datetime.strptime(fl_date_raw, '%Y%m%d %Hz')\n",
        "          print(fl_date)\n",
        "      if fl_date == valid_date_end:\n",
        "          qcdata = pd.read_csv(csv_name, skiprows=2, names=['stid', 'obs_qpf'])\n",
        "      else:\n",
        "          qcurl = f'https://www.weather.gov/source/aprfc/verification/QPEday{int(qpeday.days-1)}.csv'\n",
        "          csv_name = f'{int(qpeday.days-1)}.csv'\n",
        "          print(f'Valid url is: {qcurl}')\n",
        "          response = requests.get(qcurl)\n",
        "          with open(csv_name, 'wb') as f:\n",
        "              f.write(response.content)\n",
        "          qcdata = pd.read_csv(csv_name, skiprows=2, names=['stid', 'obs_qpf'])\n",
        "    except Exception as e:\n",
        "      print(\"Error retreiving RFC data...exiting the script\")\n",
        "      print(e)\n",
        "      qcdata = pd.DataFrame()\n",
        "    return qcdata\n",
        "\n",
        "def K_to_F(kelvin):\n",
        "  fahrenheit = 1.8*(kelvin-273)+32.\n",
        "  return fahrenheit\n",
        "\n",
        "def mps_to_kts(mps):\n",
        "  kts = mps * 1.94384\n",
        "  return kts\n",
        "\n",
        "def mm_to_in(millimeters):\n",
        "  inches = millimeters * 0.0393701\n",
        "  return inches\n",
        "\n",
        "def meters_to_in(meters):\n",
        "  inches = meters*39.3701\n",
        "  return inches\n",
        "\n",
        "def find_roots(x,y):\n",
        "  s = np.abs(np.diff(np.sign(y))).astype(bool)\n",
        "  return x[:-1][s] + np.diff(x)[s]/(np.abs(y[1:][s]/y[:-1][s])+1)\n",
        "\n",
        "\n",
        "def download_subset(remote_url, remote_file, local_filename):\n",
        "  print(\"   > Downloading a subset of NBM gribs\")\n",
        "  local_file = \"nbm/\"+local_filename\n",
        "  if \"qmd\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day max fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day min fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour min fcst:'\n",
        "    elif element == \"qpf\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':APCP:surface:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day acc fcst:'\n",
        "      else:\n",
        "        search_string = f':APCP:surface:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour acc fcst:'\n",
        "    elif element == \"maxwind\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 == 0):\n",
        "        search_string = f':WIND:10 m above ground:{str(int(nbm_qmd_forecasthour_start/24))}-{str(int(nbm_qmd_forecasthour/24))} hour max fcst:'\n",
        "      else:\n",
        "        search_string = f':WIND:10 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "  elif \"core\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      search_string = f':TMAX:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      search_string = f':TMIN:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour min fcst:'\n",
        "    elif element == \"snow\":\n",
        "      search_string = f':ASNOW:surface:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour acc'\n",
        "  #print(\"Search string = \",search_string)\n",
        "  idx = remote_url+\".idx\"\n",
        "  #print(\"IDX file = \" + idx)\n",
        "  r = requests.get(idx)\n",
        "  if not r.ok:\n",
        "    print('     ❌ SORRY! Status Code:', r.status_code, r.reason)\n",
        "    print(f'      ❌ It does not look like the index file exists: {idx}')\n",
        "\n",
        "  lines = r.text.split('\\n')\n",
        "  expr = re.compile(search_string)\n",
        "  expr\n",
        "  byte_ranges = {}\n",
        "  for n, line in enumerate(lines, start=1):\n",
        "    # n is the line number (starting from 1) so that when we call for\n",
        "    # `lines[n]` it will give us the next line. (Clear as mud??)\n",
        "    # Use the compiled regular expression to search the line\n",
        "    #print(\">> Searching throgh this line: \" + line)\n",
        "    if expr.search(line):\n",
        "      # aka, if the line contains the string we are looking for...\n",
        "      # Get the beginning byte in the line we found\n",
        "      parts = line.split(':')\n",
        "      rangestart = int(parts[1])\n",
        "      # Get the beginning byte in the next line...\n",
        "      if n+1 < len(lines):\n",
        "        # ...if there is a next line\n",
        "        parts = lines[n].split(':')\n",
        "        rangeend = int(parts[1])\n",
        "      else:\n",
        "        # ...if there isn't a next line, then go to the end of the file.\n",
        "        rangeend = ''\n",
        "\n",
        "        # Store the byte-range string in our dictionary,\n",
        "        # and keep the line information too so we can refer back to it.\n",
        "      byte_ranges[f'{rangestart}-{rangeend}'] = line\n",
        "      #print(line)\n",
        "    #else:\n",
        "      #print(\">>>  Could not find search string!\")\n",
        "  #print(\">>  Number of items in byteRange:\" + str(len(byte_ranges)))\n",
        "  for i, (byteRange, line) in enumerate(byte_ranges.items()):\n",
        "\n",
        "    if i == 0:\n",
        "      # If we are working on the first item, overwrite the existing file.\n",
        "      curl = f'curl -s --range {byteRange} {remote_url} > {local_file}'\n",
        "      #print(\">>  Adding curl command: \" + curl)\n",
        "    else:\n",
        "      # If we are working on not the first item, append the existing file.\n",
        "      curl = f'curl -s --range {byteRange} {remote_url} >> {local_file}'\n",
        "      #print(\"Adding curl command: \" + curl)\n",
        "    #print('>>  Parsing line: ' + line)\n",
        "    try:\n",
        "      num, byte, date, var, level, forecast, _ = line.split(':')\n",
        "    except:\n",
        "      pass\n",
        "      #print(\">>>  Can't get num/byte/etc from this line, so skipping...\")\n",
        "\n",
        "    #print(f'  Downloading GRIB line [{num:>3}]: variable={var}, level={level}, forecast={forecast}')\n",
        "    #print(f'  Downloading GRIB line: variable={var}, level={level}, forecast={forecast}')\n",
        "    #print(\"Running the curl command...\")\n",
        "    os.system(curl)\n",
        "\n",
        "  if os.path.exists(local_file):\n",
        "    print(f'      ✅ Success! Searched for [{search_string}] and got [{len(byte_ranges)}] GRIB fields and saved as {local_file}')\n",
        "    return local_file\n",
        "  else:\n",
        "    print(print(f'      ❌ Unsuccessful! Searched for [{search_string}] and did not find anything!'))\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "# This section for downloading and processing obs                                                                      #\n",
        "########################################################################################################################\n",
        "print('Getting obs...')\n",
        "if custom_area:\n",
        "  lonmin = float(custom_southwest.split(',')[1])\n",
        "  latmin = float(custom_southwest.split(',')[0])\n",
        "  lonmax = float(custom_northeast.split(',')[1])\n",
        "  latmax = float(custom_northeast.split(',')[0])\n",
        "obs ={}\n",
        "for region in region_list:\n",
        "  if (valid_end_datetime <= current_datetime):\n",
        "    print(\"   > Grabbing obs for: \", region)\n",
        "    #print(\"List of CWAs: \", cwa_list(region) )\n",
        "    json_name = \"obs/Obs_\"+element+\"_\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour+\"_\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour+\"_\"+region+\".json\"\n",
        "    if os.path.exists(\"obs\"):\n",
        "      pass\n",
        "    else:\n",
        "      os.mkdir(\"obs\")\n",
        "\n",
        "    #if element != \"qpf\":\n",
        "    if element == \"mint\" or element == \"maxt\":\n",
        "      api_token = \"&token=\"+synoptic_token\n",
        "      if custom_area:\n",
        "        station_query = f\"&bbox={lonmin},{latmin},{lonmax},{latmax}\"\n",
        "      else:\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "      vars_query = \"&vars=air_temp\"\n",
        "      start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "      end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "      stat_type = \"&type=\"+ob_stat\n",
        "      network_query = network_string\n",
        "      api_extras = \"&units=temp%7Cf&within=1440&status=active\"\n",
        "      obs_url = statistics_api + api_token + station_query + vars_query + start_query + end_query + stat_type + network_query + api_extras\n",
        "    elif element == \"maxwind\":\n",
        "      api_token = \"&token=\"+synoptic_token\n",
        "      if custom_area:\n",
        "        station_query = f\"&bbox={lonmin},{latmin},{lonmax},{latmax}\"\n",
        "      else:\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "      vars_query = \"&vars=wind_speed\"\n",
        "      start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "      end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "      stat_type = \"&type=\"+ob_stat\n",
        "      network_query = network_string\n",
        "      #api_extras =\n",
        "      obs_url = statistics_api + api_token + station_query + vars_query + start_query + end_query + stat_type + network_query\n",
        "    elif element == \"qpf\":\n",
        "      if use_stageiv and not use_QC_Gauge_Data:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        if custom_area:\n",
        "          station_query = f\"&bbox={lonmin},{latmin},{lonmax},{latmax}\"\n",
        "        else:\n",
        "          station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation\"\n",
        "        network_query = network_string\n",
        "        obs_url = metadata_api + api_token + station_query + network_query + api_extras\n",
        "        stageIV, s4xs, s4ys = get_stageiv()\n",
        "        s4xs, s4ys = np.meshgrid(s4xs, s4ys)\n",
        "      elif use_QC_Gauge_Data and not use_stageiv:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        if custom_area:\n",
        "          station_query = f\"&bbox={lonmin},{latmin},{lonmax},{latmax}\"\n",
        "        else:\n",
        "          station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation\"\n",
        "        network_query = network_string\n",
        "        obs_url = metadata_api + api_token + station_query + network_query + api_extras\n",
        "        print(f'Obs URL is: {obs_url}')\n",
        "        qc_df = get_qc_data()\n",
        "        #print(f'QC data is: {qc_df}')\n",
        "        is_empty = qc_df.empty\n",
        "        if is_empty:\n",
        "          sys.exit()\n",
        "      elif use_QC_Gauge_Data and use_stageiv:\n",
        "        print('You cannot use both StageIV AND QC Gauge data silly! Reverting to obs only...')\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        if custom_area:\n",
        "          station_query = f\"&bbox={lonmin},{latmin},{lonmax},{latmax}\"\n",
        "        else:\n",
        "          station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation&obtimezone=utc\"\n",
        "        network_query = network_string\n",
        "        vars_query = \"&pmode=totals\"\n",
        "        units_query = \"&units=precip|in\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        obs_url = precipitation_api + api_token + station_query + network_query + vars_query + start_query + end_query + units_query + api_extras\n",
        "      else:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        if custom_area:\n",
        "          station_query = f\"&bbox={lonmin},{latmin},{lonmax},{latmax}\"\n",
        "        else:\n",
        "          station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation&obtimezone=utc\"\n",
        "        network_query = network_string\n",
        "        print(network_string)\n",
        "        vars_query = \"&pmode=totals\"\n",
        "        units_query = \"&units=precip|in\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        obs_url = precipitation_api + api_token + station_query + network_query + vars_query + start_query + end_query + units_query + api_extras\n",
        "    elif element == \"snow\":\n",
        "      if use_nohrsc:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        if custom_area:\n",
        "          station_query = f\"&bbox={lonmin},{latmin},{lonmax},{latmax}\"\n",
        "        else:\n",
        "          station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation\"\n",
        "        network_query = network_string\n",
        "        obs_url = metadata_api + api_token + station_query + network_query + api_extras\n",
        "        snow,snowlon,snowlat = get_nohrsc()\n",
        "        snowlons,snowlats = np.meshgrid(snowlon,snowlat)\n",
        "      else:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        if custom_area:\n",
        "          station_query = f\"&bbox={lonmin},{latmin},{lonmax},{latmax}\"\n",
        "        else:\n",
        "          station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation&obtimezone=utc\"\n",
        "        network_query = network_string\n",
        "        vars_query = \"&pmode=totals\"\n",
        "        units_query = \"&units=precip|in\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        obs_url = precipitation_api + api_token + station_query + network_query + vars_query + start_query + end_query + units_query + api_extras\n",
        "    print(\"Obs url: \" + obs_url)\n",
        "    if os.path.exists(json_name):\n",
        "      print (\"Deleting old JSON file\")\n",
        "      os.remove(json_name)\n",
        "      urlretrieve(obs_url, json_name)\n",
        "    else:\n",
        "      urlretrieve(obs_url, json_name)\n",
        "\n",
        "    if os.path.exists(json_name):\n",
        "        with open(json_name) as json_file:\n",
        "            obs_json = json.load(json_file)\n",
        "            print (\"Loaded Obs JSON file line 343: \" + json_name)\n",
        "            obs_lats = []\n",
        "            obs_lons = []\n",
        "            obs_value = []\n",
        "            obs_elev = []\n",
        "            obs_stid = []\n",
        "            obs_name = []\n",
        "            for stn in obs_json[\"STATION\"]:\n",
        "                # print(stn.encode('utf-8'))\n",
        "                if stn[\"STID\"] is None:\n",
        "                  stid = \"N0N3\"\n",
        "                else:\n",
        "                  stid = stn[\"STID\"]\n",
        "                #print(f'Processing {region} station {stid}')\n",
        "                name = stn[\"NAME\"]\n",
        "                if stn[\"ELEVATION\"] and stn[\"ELEVATION\"] is not None:\n",
        "                  elev = stn[\"ELEVATION\"]\n",
        "                else:\n",
        "                  elev = -999\n",
        "                lat = stn[\"LATITUDE\"]\n",
        "                lon = stn[\"LONGITUDE\"]\n",
        "                if float(lon) > -50:\n",
        "                  continue #bug fix to deal with errant synoptic labs obs in the file\n",
        "                if element == \"mint\" or element==\"maxt\":\n",
        "                  if 'air_temp_set_1' in stn['STATISTICS'] and stn['STATISTICS']['air_temp_set_1']:\n",
        "                    if ob_stat in stn['STATISTICS']['air_temp_set_1']: # and float(stn[\"LATITUDE\"]) != 0. and float(stn[\"LONGITUDE\"]) != 0.:\n",
        "                      stat = stn['STATISTICS']['air_temp_set_1'][ob_stat]\n",
        "                      obs_stid.append(str(stid))\n",
        "                      obs_name.append(str(name))\n",
        "                      obs_elev.append(float(elev))\n",
        "                      obs_lats.append(float(lat))\n",
        "                      obs_lons.append(float(lon))\n",
        "                      obs_value.append(float(stat))\n",
        "                elif element == \"maxwind\":\n",
        "                  if 'wind_speed_set_1' in stn['STATISTICS'] and stn['STATISTICS']['wind_speed_set_1']:\n",
        "                    if ob_stat in stn['STATISTICS']['wind_speed_set_1']: # and float(stn[\"LATITUDE\"]) != 0.:\n",
        "                      stat = stn['STATISTICS']['wind_speed_set_1'][ob_stat]\n",
        "                      obs_stid.append(str(stid))\n",
        "                      obs_name.append(str(name))\n",
        "                      obs_elev.append(float(elev))\n",
        "                      obs_lats.append(float(lat))\n",
        "                      obs_lons.append(float(lon))\n",
        "                      obs_value.append(mps_to_kts(float(stat)))\n",
        "                elif (element == \"qpf\"):\n",
        "                  if (stn[\"STATUS\"] == \"ACTIVE\"): # and float(stn[\"LATITUDE\"]) < 50.924 and float(stn[\"LATITUDE\"]) > 23.377 and float(stn[\"LONGITUDE\"]) > -125.650 and float(stn[\"LONGITUDE\"]) < -66.008:\n",
        "                    obs_stid.append(str(stid))\n",
        "                    obs_name.append(str(name))\n",
        "                    obs_elev.append(float(elev))\n",
        "                    obs_lats.append(float(lat))\n",
        "                    obs_lons.append(float(lon))\n",
        "                    if use_stageiv and not use_QC_Gauge_Data:\n",
        "                      coords = project_hrap(lon, lat, s4xs, s4ys)\n",
        "                      siv_value = float(stageIV[coords])\n",
        "                      if (siv_value >= 0.01):\n",
        "                        obs_value.append(siv_value)\n",
        "                      else:\n",
        "                        obs_value.append(0.0)\n",
        "                    elif use_QC_Gauge_Data and not use_stageiv:\n",
        "                      if stn['STID'] in qc_df['stid'].values.tolist():\n",
        "                        pcp_amt = qc_df.loc[qc_df['stid'] == stn['STID'], 'obs_qpf'].iloc[0]\n",
        "                        obs_value.append(float(pcp_amt))\n",
        "                      else:\n",
        "                        obs_value.append(np.nan)\n",
        "                    elif use_QC_Gauge_Data and use_stageiv:\n",
        "                      if \"precipitation\" in stn[\"OBSERVATIONS\"]:\n",
        "                        if \"total\" in stn[\"OBSERVATIONS\"][\"precipitation\"][0]:\n",
        "                           ptotal = stn[\"OBSERVATIONS\"][\"precipitation\"][0][\"total\"]\n",
        "                           if ptotal >= 0.005:\n",
        "                              obs_value.append(ptotal)\n",
        "                           else:\n",
        "                              obs_value.append(0.0)\n",
        "                        else:\n",
        "                            obs_value.append(np.nan)\n",
        "                      else:\n",
        "                          obs_value.append(np.nan)\n",
        "                    else:\n",
        "                      if \"precipitation\" in stn[\"OBSERVATIONS\"]:\n",
        "                        if \"total\" in stn[\"OBSERVATIONS\"][\"precipitation\"][0]:\n",
        "                          ptotal = stn[\"OBSERVATIONS\"][\"precipitation\"][0][\"total\"]\n",
        "                          if ptotal >= 0.005:\n",
        "                            obs_value.append(ptotal)\n",
        "                          else:\n",
        "                            obs_value.append(0.0)\n",
        "                        else:\n",
        "                          obs_value.append(np.nan)\n",
        "                      else:\n",
        "                        obs_value.append(np.nan)\n",
        "                elif (element == \"snow\"):\n",
        "                  if stn[\"STATUS\"] == \"ACTIVE\": # and float(stn[\"LATITUDE\"]) < 50.924)and float(stn[\"LATITUDE\"]) > 23.377 and float(stn[\"LONGITUDE\"]) > -125.650 and float(stn[\"LONGITUDE\"]) < -66.008:\n",
        "                    obs_stid.append(str(stid))\n",
        "                    obs_name.append(str(name))\n",
        "                    obs_elev.append(float(elev))\n",
        "                    obs_lats.append(float(lat))\n",
        "                    obs_lons.append(float(lon))\n",
        "                    if use_nohrsc:\n",
        "                      coords = nohrsc_ll2ij(lon,lat,snowlon,snowlat)\n",
        "                      nohrsc_value = meters_to_in(float(snow[coords]))\n",
        "                      if nohrsc_value >= 0.005:\n",
        "                        obs_value.append(nohrsc_value)\n",
        "                      elif nohrsc_value < 0.0:\n",
        "                        obs_value.append(np.nan)\n",
        "                      else:\n",
        "                        obs_value.append(0.0)\n",
        "                    else:\n",
        "                      raise Exception(\"Still not able to process individual snow obs!\")\n",
        "            csv_name = \"obs_\"+element+\"_\"+region+\".csv\"\n",
        "            obs[region] = pd.DataFrame()\n",
        "            obs[region][\"stid\"] = obs_stid\n",
        "            obs[region][\"name\"] = obs_name\n",
        "            obs[region][\"elevation\"] = obs_elev\n",
        "            obs[region][\"lat\"] = obs_lats\n",
        "            obs[region][\"lon\"] = obs_lons\n",
        "            obs[region][\"ob_\"+element] = obs_value\n",
        "            obs[region].dropna(inplace=True)\n",
        "            if elev_filter and filter_above_below == \"above\":\n",
        "              obs[region] = obs[region][obs[region][\"elevation\"] <= elev_filter_value]\n",
        "            if elev_filter and filter_above_below == \"below\":\n",
        "              obs[region] = obs[region][obs[region][\"elevation\"] >= elev_filter_value]\n",
        "            obs[region].to_csv(csv_name)\n",
        "  else:\n",
        "    print(f'    > Valid Time in the future. Grabbing obs points only for: {region}')\n",
        "    json_name = \"obs/ObsPoints_\"+region+\"_wcoss.json\"\n",
        "    if os.path.exists(json_name):\n",
        "      pass\n",
        "    else:\n",
        "      if os.path.exists(\"obs\"):\n",
        "        pass\n",
        "      else:\n",
        "        os.mkdir(\"obs\")\n",
        "      obs_url = \"https://api.synopticdata.com/v2/stations/metadata?&token=\"+synoptic_token+\"&cwa=\"+cwa_list(region)+\"&fields=status,latitude,longitude,name,elevation\"+network_string\n",
        "      urlretrieve(obs_url, json_name)\n",
        "    if os.path.exists(json_name):\n",
        "      with open(json_name) as json_file:\n",
        "          obs_json = json.load(json_file)\n",
        "          print(\"Loaded Obs JSON file line 427!\")\n",
        "          obs_lats = []\n",
        "          obs_lons = []\n",
        "          obs_elev = []\n",
        "          obs_stid = []\n",
        "          obs_name = []\n",
        "          for stn in obs_json[\"STATION\"]:\n",
        "            # print(stn.encode('utf-8'))\n",
        "            if stn[\"STID\"] is None:\n",
        "              stid = \"N0N3\"\n",
        "            else:\n",
        "              stid = stn[\"STID\"]\n",
        "            #print(f'Processing {region} station {stid}')\n",
        "            name = stn[\"NAME\"]\n",
        "            if stn[\"ELEVATION\"] and stn[\"ELEVATION\"] is not None:\n",
        "              elev = stn[\"ELEVATION\"]\n",
        "            else:\n",
        "              elev = -999\n",
        "            lat = stn[\"LATITUDE\"]\n",
        "            lon = stn[\"LONGITUDE\"]\n",
        "            if stn[\"STATUS\"] == \"ACTIVE\": # and float(stn[\"LATITUDE\"]) != 0. and float(stn[\"LONGITUDE\"]) != 0.:\n",
        "              obs_stid.append(str(stid))\n",
        "              obs_name.append(str(name))\n",
        "              obs_elev.append(float(elev))\n",
        "              obs_lats.append(float(lat))\n",
        "              obs_lons.append(float(lon))\n",
        "          obs[region] = pd.DataFrame()\n",
        "          obs[region][\"stid\"] = obs_stid\n",
        "          obs[region][\"name\"] = obs_name\n",
        "          obs[region][\"elevation\"] = obs_elev\n",
        "          obs[region][\"lat\"] = obs_lats\n",
        "          obs[region][\"lon\"] = obs_lons\n",
        "          obs[region][\"ob_\"+element] = -999\n",
        "          obs[region].dropna(inplace=True)\n",
        "          #obs[region].to_csv(csv_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0BRckH750J8",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 4. Download Gribs and Interpolate\n",
        "########################################################################################################################\n",
        "# This section downloads and processes the NBM.                                                                        #\n",
        "########################################################################################################################\n",
        "if \"AR\" in region_list or \"AJK\" in region_list or \"ARH\" in region_list or \"AFC\" in region_list or \"AFG\" in region_list:\n",
        "  rg=\"ak\"\n",
        "elif \"HFO\" in region_list:\n",
        "  rg=\"hi\"\n",
        "elif \"SJU\" in region_list:\n",
        "  rg=\"pr\"\n",
        "else:\n",
        "  rg=\"co\"\n",
        "print(f'Region list is: {region_list}')\n",
        "print(f'Region is: {rg}')\n",
        "print('Getting and processing NBM...')\n",
        "nbm_init_filen = nbm_init.strftime('%Y%m%d') + \"_\" + nbm_init.strftime('%H')\n",
        "nbm_init_filen_core = core_init.strftime('%Y%m%d') + \"_\" + core_init.strftime('%H')\n",
        "if nbm_version == \"4.3\":\n",
        "  nbm_url_base = \"https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.\"+nbm_init.strftime('%Y%m%d') \\\n",
        "              +\"/\"+nbm_init.strftime('%H')+\"/\"\n",
        "  nbm_url_base_core = \"https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.\"+core_init.strftime('%Y%m%d') \\\n",
        "              +\"/\"+core_init.strftime('%H')+\"/\"\n",
        "else:\n",
        "  nbm_url_base = \"https://noaa-nbm-para-pds.s3.amazonaws.com/blend.\"+nbm_init.strftime('%Y%m%d') \\\n",
        "              +\"/\"+nbm_init.strftime('%H')+\"/\"\n",
        "  nbm_url_base_core = \"https://noaa-nbm-para-pds.s3.amazonaws.com/blend.\"+core_init.strftime('%Y%m%d') \\\n",
        "              +\"/\"+core_init.strftime('%H')+\"/\"\n",
        "temp_vars = [\"maxt\",\"mint\"]\n",
        "if (element == \"qpf\"):\n",
        "  detr_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.{rg}.grib2'\n",
        "  if nbm_version == \"4.3\":\n",
        "    detr_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.{rg}.{element}_subset.grib2'\n",
        "  else:\n",
        "    detr_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.{rg}.{element}_subset_exp.grib2'\n",
        "  detr_url = nbm_url_base+\"qmd/\"+detr_file\n",
        "\n",
        "elif any(te in element for te in temp_vars):\n",
        "  detr_file = f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.{rg}.grib2\"\n",
        "  if nbm_version == \"4.3\":\n",
        "    detr_file_subset = f\"blend.t{int(core_init.strftime('%H')):02}z.core.{nbm_init_filen_core}f{int(nbm_core_forecasthour):03}.{rg}.{element}_subset.grib2\"\n",
        "  else:\n",
        "    detr_file_subset = f\"blend.t{int(core_init.strftime('%H')):02}z.core.{nbm_init_filen_core}f{int(nbm_core_forecasthour):03}.{rg}.{element}_subset_exp.grib2\"\n",
        "  detr_url = nbm_url_base_core+\"core/\"+detr_file\n",
        "\n",
        "elif element == \"maxwind\" or element == \"maxgust\":\n",
        "  print(\"MAXWIND: No Core data available!\")\n",
        "  #core_init = nbm_init\n",
        "  #nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "\n",
        "elif element == \"snow\":\n",
        "  #for snow, even for prob/perecntiles, we are dealing with a core file\n",
        "  detr_file=f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.{rg}.grib2\"\n",
        "  if nbm_version == \"4.3\":\n",
        "    detr_file_subset = f\"blend.t{int(core_init.strftime('%H')):02}z.core.{nbm_init_filen_core}f{int(nbm_core_forecasthour):03}.{rg}.{element}_subset.grib2\"\n",
        "  else:\n",
        "    detr_file_subset = f\"blend.t{int(core_init.strftime('%H')):02}z.core.{nbm_init_filen_core}f{int(nbm_core_forecasthour):03}.{rg}.{element}_subset_exp.grib2\"\n",
        "  detr_url = nbm_url_base_core+\"core/\"+detr_file\n",
        "  #raise Exception (\"Not ready to deal with snow deterministic yet!\")\n",
        "  #print(\"SNOW: No Core data avaialable!\")\n",
        "\n",
        "if element != \"maxwind\" and element != \"maxgust\": # and element != \"snow\":\n",
        "  if os.path.exists(\"nbm/\"+detr_file_subset):\n",
        "    print(\"   > NBM deterministic already exists\")\n",
        "  else:\n",
        "    print(\"   > Getting NBM deterministic\")\n",
        "    if os.path.exists(\"nbm\"):\n",
        "      pass\n",
        "    else:\n",
        "      os.mkdir(\"nbm\")\n",
        "    #urlretrieve(detr_url, \"nbm/\"+detr_file)\n",
        "    download_subset(detr_url, detr_file, detr_file_subset)\n",
        "  #print(detr_url)\n",
        "  nbmd = pygrib.open(\"nbm/\"+detr_file_subset)\n",
        "  if element == \"maxt\":\n",
        "    deterministic = nbmd.select(name=\"Maximum temperature\",lengthOfTimeRange=12, stepTypeInternal=\"max\")[0]\n",
        "    deterministic_array = K_to_F(deterministic.values)\n",
        "  elif element == \"mint\":\n",
        "    deterministic = nbmd.select(name=\"Minimum temperature\",lengthOfTimeRange=12, stepTypeInternal=\"min\")[0]\n",
        "    deterministic_array = K_to_F(deterministic.values)\n",
        "  elif element == \"qpf\":\n",
        "    deterministic = nbmd.select(name=\"Total Precipitation\",lengthOfTimeRange=24)[-1]\n",
        "    deterministic_array = mm_to_in(deterministic.values)\n",
        "  if element == \"snow\":\n",
        "    deterministic = nbmd.select(name=\"unknown\",lengthOfTimeRange=24)[7] #look at 8th record\n",
        "    deterministic_array = mm_to_in(deterministic.values)\n",
        "  nbmlats, nbmlons = deterministic.latlons()\n",
        "  nbmd.close()\n",
        "\n",
        "  for region in region_list:\n",
        "    print(\"     >> Extracting NBM deterministic\")\n",
        "    point_lats = obs[region][\"lat\"].values\n",
        "    point_lons = obs[region][\"lon\"].values\n",
        "    detr_values = []\n",
        "    nbm_fidx = []\n",
        "    for i in range(0, len(point_lats)):\n",
        "      coords = ll_to_index(nbmlons, nbmlats, point_lons[i], point_lats[i])\n",
        "      detr_value = deterministic_array[coords]\n",
        "      nbm_fidx.append(coords)\n",
        "      if (element == \"qpf\") and detr_value < 0.005: #set very light values to zero\n",
        "        detr_value = 0.0\n",
        "      detr_values.append(detr_value)\n",
        "    obs[region][\"NBM_fidx\"] = nbm_fidx\n",
        "    obs[region][\"NBM_D\"] = detr_values\n",
        "else:\n",
        "  #set up forecast dataframe for wind/gust, but make determinstic values nan\n",
        "  print(\"MAXWIND: Putting station locations into dataframe\")\n",
        "  #fcst[cwa]=pd.DataFrame()\n",
        "  point_lats = obs[region][\"lat\"].values\n",
        "  point_lons = obs[region][\"lon\"].values\n",
        "  nbm_fidx = []\n",
        "  nbmlats=None\n",
        "  nbmlons=None\n",
        "  stations = obs[region][\"stid\"]\n",
        "  detr_values=np.empty(np.shape(stations))\n",
        "  detr_values.fill(np.nan)\n",
        "  obs[region][\"NBM_D\"] = detr_values\n",
        "\n",
        "if nbm_version == \"4.3\":\n",
        "  if element == \"snow\":\n",
        "    perc_list = [5,10,25,50,75,90,95]\n",
        "  else:\n",
        "    perc_list = [1,5,10,20,30,40,50,60,70,80,90,95,99]\n",
        "else:\n",
        "  if element == \"snow\":\n",
        "    perc_list = [5,10,25,50,75,90,95]\n",
        "  else:\n",
        "    perc_list = [0,5,10,20,30,40,50,60,70,80,90,95,100]\n",
        "#perc_list = range(1,100,1)\n",
        "perc_dict = {\"maxt\":\"maxt18p\", \"mint\":\"mint18p\", \"qpf\":\"qpf24p\", \"snow\":\"snow24p\"}\n",
        "if element == \"snow\":\n",
        "  if nbm_version == \"4.3\":\n",
        "    perc_file=f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.{rg}.grib2\"\n",
        "  else:\n",
        "    perc_file=f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.{rg}_exp.grib2\"\n",
        "  dummy_file = f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.{rg}.grib2\"\n",
        "  perc_url=nbm_url_base_core+\"core/\"+dummy_file\n",
        "else:\n",
        "  if nbm_version == \"4.3\":\n",
        "    perc_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.{rg}.grib2'\n",
        "  else:\n",
        "    perc_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.{rg}_exp.grib2'\n",
        "  dummy_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.{rg}.grib2'\n",
        "  perc_url = nbm_url_base+\"qmd/\"+dummy_file\n",
        "if nbm_version == \"4.3\":\n",
        "  perc_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.{rg}.{element}_subset.grib2'\n",
        "else:\n",
        "  perc_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.{rg}.{element}_subset_exp.grib2'\n",
        "print(\"perc_url=\",perc_url)\n",
        "print(\"perc_file=\",perc_file)\n",
        "if os.path.exists(\"nbm\"):\n",
        "  pass\n",
        "else:\n",
        "  os.mkdir(\"nbm\")\n",
        "if os.path.exists(\"nbm/\"+perc_file_subset):\n",
        "  print(\"   > NBM probabilistic already exists\")\n",
        "else:\n",
        "  #urlretrieve(perc_url, \"nbm/\"+perc_file)\n",
        "  print(\"   > Getting NBM probabilistic\")\n",
        "  download_subset(perc_url, perc_file, perc_file_subset)\n",
        "\n",
        "\n",
        "\n",
        "def select_from_prob_msgs(prob_msgs, **criteria):\n",
        "    matches = []\n",
        "    for g in prob_msgs:\n",
        "        #print(g.keys())\n",
        "        #print(f\"Grib we are looking for is: {g.name}\")\n",
        "        try:\n",
        "            if all(getattr(g, k) == v for k, v in criteria.items()):\n",
        "                matches.append(g)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return matches\n",
        "#nbmperc = pygrib.open(\"nbm/\"+perc_file_subset)\n",
        "all_msgs = pygrib.open(\"nbm/\" + perc_file_subset)\n",
        "# Filter messages that have a percentileValue key\n",
        "prob_msgs = [g for g in all_msgs if \"percentileValue\" in g.keys()]\n",
        "#print(f\"Prob messages are: {prob_msgs}\")\n",
        "\n",
        "print(f\"Filtered {len(prob_msgs)} probabilistic messages out of {all_msgs.messages} total\")\n",
        "\n",
        "#for i, g in enumerate(nbmperc):\n",
        "#    print(f\"\\nMessage #{i + 1}\")\n",
        "#    print(\"Name:\", g.name)\n",
        "#    print(\"Available keys:\")\n",
        "#    for key in g.keys():\n",
        "#        print(f\"  {key}\")\n",
        "print('   > Extracting NBM Probabilistic')\n",
        "for perc in perc_list:\n",
        "  print(f'     >> Extracting NBM P{int(perc):01}')\n",
        "  perc_name = \"NBM_P\"+str(perc)\n",
        "  if element == \"maxt\":\n",
        "    try:\n",
        "      percdata = K_to_F(select_from_prob_msgs(prob_msgs, name=\"Time-maximum 2 metre temperature\", stepTypeInternal=\"max\", percentileValue=perc)[0].values)\n",
        "    except IndexError:\n",
        "      #percdata = K_to_F(select_from_prob_msgs(prob_msgs, name=\"2 metre temperature\", stepTypeInternal=\"max\", percentileValue=perc)[0].values)\n",
        "      percdata = K_to_F(select_from_prob_msgs(prob_msgs, name=\"Maximum temperature at 2 metres since previous post-processing\", stepTypeInternal=\"max\", percentileValue=perc)[0].values)\n",
        "    except IndexError:\n",
        "      percdata = K_to_F(select_from_prob_msgs(prob_msgs, name=\"2 metre temperature\", stepTypeInternal=\"max\", percentileValue=perc)[0].values)\n",
        "  elif element == \"mint\":\n",
        "    try:\n",
        "      percdata = K_to_F(select_from_prob_msgs(prob_msgs, name=\"Time-minimum 2 metre temperature\", stepTypeInternal=\"min\", percentileValue=perc)[0].values)\n",
        "    except IndexError:\n",
        "      percdata = K_to_F(select_from_prob_msgs(prob_msgs, name=\"Minimum temperature at 2 metres since previous post-processing\", stepTypeInternal=\"min\", percentileValue=perc)[0].values)\n",
        "    except IndexError:\n",
        "      percdata = K_to_F(select_from_prob_msgs(prob_msgs, name=\"2 metre temperature\", stepTypeInternal=\"min\", percentileValue=perc)[0].values)\n",
        "  elif element == \"qpf\":\n",
        "    percdata = mm_to_in(select_from_prob_msgs(prob_msgs, name=\"Total Precipitation\",lengthOfTimeRange=24, percentileValue=perc)[0].values)\n",
        "  elif element == \"maxwind\":\n",
        "    try:\n",
        "      percinv = select_from_prob_msgs(prob_msgs,name=\"10 metre wind speed\",lengthOfTimeRange=24,stepTypeInternal=\"max\",percentileValue=perc)[0]\n",
        "    except IndexError:\n",
        "      percinv = select_from_prob_msgs(prob_msgs,name=\"Time-maximum 10 metre wind speed\",lengthOfTimeRange=24,percentileValue=perc)[0]\n",
        "    percdata = mps_to_kts(percinv.values)\n",
        "  elif element == \"snow\":\n",
        "    percinv = select_from_prob_msgs(prob_msgs, name=\"unknown\",lengthOfTimeRange=24,percentileValue=perc)[0]\n",
        "    percdata=meters_to_in(percinv.values)\n",
        "  if nbmlats is None:\n",
        "    nbmlats,nbmlons=percinv.latlons()\n",
        "    for i in range(0,len(point_lats)):\n",
        "      coords = ll_to_index(nbmlons,nbmlats,point_lons[i],point_lats[i])\n",
        "      nbm_fidx.append(coords)\n",
        "    obs[region][\"NBM_fidx\"] = nbm_fidx\n",
        "  for region in region_list:\n",
        "    nbm_coords = obs[region][\"NBM_fidx\"].values\n",
        "    perc_values = []\n",
        "    for i in range(0, len(nbm_coords)):\n",
        "      perc_value = percdata[nbm_coords[i]]\n",
        "      perc_values.append(perc_value)\n",
        "    obs[region][perc_name] = perc_values\n",
        "all_msgs.close()\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "# This section creates a distribution curve at each site, and interpolates ob and deterministic to percentile space    #\n",
        "########################################################################################################################\n",
        "print('Creating point distribution curves and interpolating...')\n",
        "if nbm_version == \"4.3\":\n",
        "  start_loc = \"NBM_P1\"\n",
        "  end_loc = \"NBM_P99\"\n",
        "else:\n",
        "  start_loc = \"NBM_P0\"\n",
        "  end_loc = \"NBM_P100\"\n",
        "for region in region_list:\n",
        "  if element == \"snow\":\n",
        "    perc_start = obs[region].columns.get_loc(\"NBM_P5\")\n",
        "    perc_end = obs[region].columns.get_loc(\"NBM_P95\")\n",
        "    all_percs = obs[region].iloc[:, perc_start:perc_end+1].values\n",
        "  else:\n",
        "    perc_start = obs[region].columns.get_loc(start_loc)\n",
        "    perc_end = obs[region].columns.get_loc(end_loc)\n",
        "    all_percs = obs[region].iloc[:, perc_start:perc_end+1].values\n",
        "  var_string = \"ob_\"+element\n",
        "  all_obs = obs[region][[var_string]].values\n",
        "  all_nbmd = obs[region][['NBM_D']].values\n",
        "  obs_percs = []\n",
        "  nbmd_percs = []\n",
        "  for i in range(0,len(all_obs)):\n",
        "    udf = us(perc_list, all_percs[i,:], bbox=[0,100], ext=0, s=0)\n",
        "    if np.isnan(all_obs[i]):\n",
        "      ob_perc = np.nan\n",
        "    elif all_obs[i] == 0.0 and (element == \"qpf\" or element == \"snow\"):\n",
        "        zo=len(perc_list)-np.count_nonzero(all_percs[i,:])\n",
        "        if zo == 0:\n",
        "          ob_perc = -10\n",
        "        elif zo == 1:\n",
        "          ob_perc = 10\n",
        "        else:\n",
        "          ob_perc = np.random.randint(low=1,high=perc_list[zo-1])\n",
        "    elif all_obs[i] < udf(0):\n",
        "      ob_perc = -10\n",
        "    elif all_obs[i] > udf(100):\n",
        "      ob_perc = 110\n",
        "    else:\n",
        "      ob_perc = find_roots(np.arange(0,101,1), udf(np.arange(0,101,1)) - all_obs[i])\n",
        "      ob_perc = ob_perc[0].round(1)\n",
        "\n",
        "    if np.isnan(all_nbmd[i]):\n",
        "      nbm_perc = np.nan\n",
        "    elif all_nbmd[i] == 0.0 and (element == \"qpf\" or element == \"snow\"):\n",
        "      zn=len(perc_list)-np.count_nonzero(all_percs[i,:])\n",
        "      if zn == 0:\n",
        "        nbm_perc = -10\n",
        "      elif zn == 1:\n",
        "        nbm_perc = 10\n",
        "      else:\n",
        "        #print(f'zn is: {zn} and zo -1 is : {zo-1}')\n",
        "        #print(f'percent list is: {perc_list}')\n",
        "        nbm_perc = np.random.randint(low=1,high=perc_list[zn-1])\n",
        "        #print(f' NBM percentile is: {nbm_perc}')\n",
        "    elif all_nbmd[i] < udf(0):\n",
        "      nbm_perc = -10\n",
        "    elif all_nbmd[i] > udf(100):\n",
        "      nbm_perc = 110\n",
        "    else:\n",
        "      nbm_perc = find_roots(np.arange(0,101,1), udf(np.arange(0,101,1)) - all_nbmd[i])\n",
        "      nbm_perc = nbm_perc[0].round(1)\n",
        "\n",
        "    if np.isnan(ob_perc):\n",
        "      obs_percs.append(ob_perc)\n",
        "    else:\n",
        "      obs_percs.append(int(ob_perc))\n",
        "    if np.isnan(nbm_perc):\n",
        "      nbmd_percs.append(nbm_perc)\n",
        "    else:\n",
        "      nbmd_percs.append(int(nbm_perc))\n",
        "  obs[region][\"ob_perc\"] = obs_percs\n",
        "  obs[region][\"NBMd_perc\"] = nbmd_percs\n",
        "  if export_csv:\n",
        "    csv_name = \"obs_and_percs_\"+element+\"_\"+nbm_init.strftime('%Y%m%d')+\"_\"+valid_end_datetime.strftime('%Y%m%d')+\"_\"+region+\".csv\"\n",
        "    obs[region].to_csv(csv_name)\n",
        "    print(f'   > Created and saved {csv_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC8a2EWZ5j_s",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 5.  Generate Plot\n",
        "########################################################################################################################\n",
        "# Finally, this section makes our plot.                                                                                #\n",
        "########################################################################################################################\n",
        "print(\"Making plot (almost done!)...\")\n",
        "if compare_to ==\"obs\":\n",
        "  compare_var = \"ob_perc\"\n",
        "  compare_element = \"Obs\"\n",
        "elif compare_to == \"deterministic\":\n",
        "  compare_var = \"NBMd_perc\"\n",
        "  if element ==\"qpf\":\n",
        "    compare_element = \"pMean\"\n",
        "  else:\n",
        "    compare_element = \"Detr\"\n",
        "\n",
        "title_dict = {\"maxt\":[\"Max T\",\"PMaxT\"],\"mint\":[\"Min T\",\"PMinT\"], \"qpf\":[\"QPF\",\"PQPF\"], \"maxwind\":[\"Max Wind\",\"Prob Max Wind\"], \"snow\":[\"Snow Acc\", \"Prob Snow Acc\"]}\n",
        "matplotlib.rc('axes',facecolor=background_color, edgecolor=text_color)\n",
        "if (element == \"qpf\" or element == \"snow\"):\n",
        "  valid_datetime = valid_date\n",
        "  fig_valid_date = nbm_qmd_valid_end_datetime.strftime('%Y%m%d_%HZ')\n",
        "  valid_title = nbm_qmd_valid_end_datetime.strftime('%HZ %a %m-%d-%Y')\n",
        "else:\n",
        "  valid_datetime = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "  fig_valid_date = valid_datetime.strftime('%Y%m%d')\n",
        "  valid_title = valid_datetime.strftime('%a %m-%d-%Y')\n",
        "if (element == \"snow\"):\n",
        "  nbm_init_title = core_init.strftime('%HZ %m-%d-%Y')\n",
        "else:\n",
        "  nbm_init_title = nbm_init.strftime('%HZ %m-%d-%Y')\n",
        "\n",
        "def flip(items, ncol):\n",
        "    return itertools.chain(*[items[i::ncol] for i in range(ncol)])\n",
        "\n",
        "if region_selection == \"CONUS\":\n",
        "  dataframeid = \"CONUS\"\n",
        "  #set up multipanel plot\n",
        "  west =-125.650\n",
        "  south = 23.377\n",
        "  east = -66.008\n",
        "  north = 50.924\n",
        "  width_ratios = [7,3,3,3]\n",
        "  lloc = \"lower right\"\n",
        "  fig = plt.figure(constrained_layout=True, figsize=(16,9), facecolor=background_color, frameon=True, dpi=150)\n",
        "  grid = fig.add_gridspec(4,4, width_ratios=width_ratios, hspace=0.2, wspace=0.2, left=0.1, right=0.9)\n",
        "  fig.text(0.30, 0.885,f'{region_selection} {title_dict[element][0]} {compare_element} in NBM {nbm_version} Percentile Space',horizontalalignment='center',weight='bold',fontsize=25,color=text_color)\n",
        "  fig.text(0.30, 0.855,f'Valid: {valid_title}  |  NBM Init: {nbm_init_title}  |  Points: {points_str}',horizontalalignment='center',fontsize=16,color=text_color)\n",
        "\n",
        "  ax1 = fig.add_subplot(grid[:,:-2], projection=ccrs.Mercator(globe=None))\n",
        "  ax2 = fig.add_subplot(grid[0,2])\n",
        "  ax3 = fig.add_subplot(grid[0,3])\n",
        "  ax4 = fig.add_subplot(grid[1,2])\n",
        "  ax5 = fig.add_subplot(grid[1,3])\n",
        "  ax6 = fig.add_subplot(grid[2:,2:])\n",
        "\n",
        "  conus_df = pd.concat([obs[\"WR\"], obs[\"CR\"], obs[\"ER\"],obs[\"SR\"]])\n",
        "  lats = conus_df[\"lat\"].values\n",
        "  lons = conus_df[\"lon\"].values\n",
        "  point_data = conus_df[compare_var].values\n",
        "  mean = conus_df[compare_var].mean()\n",
        "  median = conus_df[compare_var].median()\n",
        "  mode = conus_df[compare_var].mode().values[0]\n",
        "\n",
        "  proj = ccrs.PlateCarree()\n",
        "\n",
        "  ax1.set_anchor('S')\n",
        "  ax1.set_extent([west, east, south, north], crs=proj)\n",
        "  #ax1.add_feature(cfeature.LAND, edgecolor='none', facecolor='#414143', zorder=-1)\n",
        "  ax1.add_feature(cfeature.OCEAN, edgecolor='none', facecolor=map_water_color, zorder=-2)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '50m', edgecolor='none', facecolor=map_land_color, zorder=-1))\n",
        "  #ax1.add_feature(cfeature.LAKES, edgecolor='none', facecolor='#272727', zorder=0)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'lakes', '10m', edgecolor='none', facecolor=map_water_color, zorder=0))\n",
        "  ax1.add_feature(cfeature.BORDERS, edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=1)\n",
        "  #ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'countries', '50m', edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=2))\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'admin_1_states_provinces_lines', '50m', edgecolor=map_border_color, facecolor='none', linewidth=1, zorder=2))\n",
        "  #cx.add_basemap(ax1, source='https://server.arcgisonline.com/ArcGIS/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}', attribution=False)\n",
        "  scatter = ax1.scatter(lons, lats, c= point_data, cmap=cmap, s=45, transform=proj, vmin=0.0, vmax=100.0)\n",
        "  if plot_cities:\n",
        "    plot_towns(ax1, south, north, west, east, population=pop_thresh)\n",
        "  #handles, labels = scatter.legend_elements(num=10)\n",
        "  #legend1 = ax1.legend(flip(handles, 6), flip(labels, 6), ncol=6,loc=lloc, title=f'{compare_element} in NBM Percentile Space', fancybox=True)\n",
        "  numcols=abs(np.amax(point_data) - np.amin(point_data))//10\n",
        "  legend1 = ax1.legend(*scatter.legend_elements(num=numcols), loc=lloc, title=f'{compare_element} \\n Rank', fancybox=True)\n",
        "  plt.setp(legend1.get_title(), multialignment='center', color=text_color)\n",
        "  for text in legend1.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax1.add_artist(legend1)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature(\n",
        "    'cultural', 'admin_1_states_provinces_lines', '110m',\n",
        "    edgecolor='gray', facecolor='none'))\n",
        "  if cwa_outline:\n",
        "    try:\n",
        "      if os.path.exists(\"shp/w_22mr22.shp\"):\n",
        "        pass\n",
        "      else:\n",
        "        cwa_url = \"https://www.weather.gov/source/gis/Shapefiles/WSOM/w_22mr22.zip\"\n",
        "        os.mkdir(\"shp\")\n",
        "        urlretrieve(cwa_url, \"shp/nws_cwa_outlines.zip\")\n",
        "        #!unzip shp/nws_cwa_outlines.zip -d shp\n",
        "        with zipfile.ZipFile(\"shp/nws_cwa_outlines.zip\", 'r') as zip_ref:\n",
        "          zip_ref.extractall(\"shp\")\n",
        "      cwa_feature = ShapelyFeature(Reader(\"shp/w_22mr22.shp\").geometries(),ccrs.PlateCarree(), edgecolor='grey', facecolor='none', linewidth=0.5, linestyle=':', zorder=3)\n",
        "      ax1.add_feature(cwa_feature)\n",
        "    except:\n",
        "      print(\"   > Aw shucks, no CWA boundaries for you. Sorry bout that.\")\n",
        "  if county_outline:\n",
        "    try:\n",
        "      if os.path.exists(\"shp/c_08mr23.zip\"):\n",
        "        pass\n",
        "      else:\n",
        "        county_url = \"https://www.weather.gov/source/gis/Shapefiles/County/c_08mr23.zip\"\n",
        "        os.mkdir(\"shp\")\n",
        "        urlretrieve(county_url, \"shp/counties.zip\")\n",
        "        with zipfile.ZipFile(\"shp/counties.zip\",'r') as zip_ref:\n",
        "          zip_ref.extractall(\"shp\")\n",
        "      cty_feature = ShapelyFeature(Reader(\"shp/c_08mr23.shp\").geometries().ccrs.PlateCarree(), edgecolor='white',facecolor='none',linewidth=1.0,linestyle=\"--\",zorder=3)\n",
        "      ax1.add_feature(cty_feature)\n",
        "    except:\n",
        "      print(\"   > Cannot plot county boundaries.\")\n",
        "\n",
        "  mean_wr = obs[\"WR\"][compare_var].mean()\n",
        "  median_wr = obs[\"WR\"][compare_var].median()\n",
        "  #mode_wr = obs[\"WR\"][compare_var].mode().values[0]\n",
        "  ax2.set_anchor('N')\n",
        "  sns.histplot(data=obs[\"WR\"], x=compare_var, ax=ax2, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax2.set_xlabel(\"Western Region\", color=text_color, fontsize=12)\n",
        "  ax2.axvline(mean_wr, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax2.axvline(median_wr, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "  #ax2.axvline(mode_wr, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "  ax2.grid(False)\n",
        "  for tick in ax2.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax2.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax2.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend2 = ax2.legend()\n",
        "  for text in legend2.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax2.set(ylabel=None)\n",
        "\n",
        "  mean_cr = obs[\"CR\"][compare_var].mean()\n",
        "  median_cr = obs[\"CR\"][compare_var].median()\n",
        "  #mode_cr = obs[\"CR\"][compare_var].mode().values[0]\n",
        "  ax3.set_anchor('N')\n",
        "  sns.histplot(data=obs[\"CR\"], x=compare_var, ax=ax3, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax3.set_xlabel(\"Central Region\", color=text_color, fontsize=12)\n",
        "  ax3.axvline(mean_cr, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax3.axvline(median_cr, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "  #ax3.axvline(mode_cr, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "  ax3.grid(False)\n",
        "  for tick in ax3.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax3.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax3.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend3 = ax3.legend()\n",
        "  for text in legend3.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax3.set(ylabel=None)\n",
        "\n",
        "\n",
        "  mean_er = obs[\"ER\"][compare_var].mean()\n",
        "  median_er = obs[\"ER\"][compare_var].median()\n",
        "  #mode_er = obs[\"ER\"][compare_var].mode().values[0]\n",
        "  ax4.set_anchor('N')\n",
        "  sns.histplot(data=obs[\"ER\"], x=compare_var, ax=ax4, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax4.set_xlabel(\"Eastern Region\", color=text_color, fontsize=12)\n",
        "  ax4.axvline(mean_er, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax4.axvline(median_er, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "  #ax4.axvline(mode_er, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "  ax4.grid(False)\n",
        "  for tick in ax4.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax4.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax4.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend4 = ax4.legend()\n",
        "  for text in legend4.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax4.set(ylabel=None)\n",
        "\n",
        "\n",
        "  mean_sr = obs[\"SR\"][compare_var].mean()\n",
        "  median_sr = obs[\"SR\"][compare_var].median()\n",
        "  #mode_sr = obs[\"SR\"][compare_var].mode().values[0]\n",
        "  ax5.set_anchor('N')\n",
        "  sns.histplot(data=obs[\"SR\"], x=compare_var, ax=ax5, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax5.set_xlabel(\"Southern Region\", color=text_color, fontsize=12)\n",
        "  ax5.axvline(mean_sr, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax5.axvline(median_sr, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "  #ax5.axvline(mode_sr, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "  ax5.grid(False)\n",
        "  for tick in ax5.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax5.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax5.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend5 = ax5.legend()\n",
        "  for text in legend5.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax5.set(ylabel=None)\n",
        "\n",
        "  ax6.set_anchor('NC')\n",
        "  sns.histplot(data=point_data, ax=ax6, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax6.set_xlabel(f'{compare_element} in NBM {title_dict[element][1]} Percentile Bins', color=text_color, fontsize=12)\n",
        "  ax6.axvline(mean, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax6.axvline(median, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "  #ax6.axvline(mode, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "  ax6.grid(False)\n",
        "  for tick in ax6.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax6.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax6.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend6 = ax6.legend()\n",
        "  for text in legend6.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax6.set(ylabel=None)\n",
        "\n",
        "  #figname=region_selection+\"_\"+element+\"_\"+valid_date+\".png\"\n",
        "  #plt.savefig(figname, facecolor=fig.get_facecolor(), bbox_inches=None, pad_inches=0.2, dpi='figure')\n",
        "\n",
        "else:\n",
        "  if not custom_area:\n",
        "    #set up two panel plot\n",
        "    if (region_selection == \"AR\"):\n",
        "        west = -179.00\n",
        "        south = 52.00\n",
        "        east = -129.00\n",
        "        north = 72.00\n",
        "        width, height = (16,7)\n",
        "        width_ratios = [9,7]\n",
        "        lloc = \"lower left\"\n",
        "    if (region_selection == \"WR\"):\n",
        "        west = -126.917\n",
        "        south = 30.586\n",
        "        east = -102.740\n",
        "        north = 49.755\n",
        "        width, height = (16,9)\n",
        "        width_ratios = [9,8]\n",
        "        lloc = \"lower right\"\n",
        "    if (region_selection == \"CR\"):\n",
        "        west = -111.534\n",
        "        south = 33.295\n",
        "        east = -81.723\n",
        "        north = 49.755\n",
        "        width, height = (16,7)\n",
        "        width_ratios = [9,7]\n",
        "        lloc = \"lower center\"\n",
        "    if (region_selection == \"ER\"):\n",
        "        west = -86.129\n",
        "        south = 31.223\n",
        "        east = -66.465\n",
        "        north = 47.676\n",
        "        width, height = (16,7.25)\n",
        "        width_ratios = [6.9,9.5]\n",
        "        lloc = \"lower right\"\n",
        "    if (region_selection == \"SR\"):\n",
        "        west = -109.758\n",
        "        south = 23.313\n",
        "        east = -79.247\n",
        "        north = 36.899\n",
        "        width, height = (16,5.6)\n",
        "        width_ratios = [10,6]\n",
        "        lloc = \"lower center\"\n",
        "    if (region_selection == \"CWA\"):\n",
        "        west = np.min(obs[region][\"lon\"]) - 0.5\n",
        "        south = np.min(obs[region][\"lat\"]) - 0.5\n",
        "        east = np.max(obs[region][\"lon\"]) + 1.0\n",
        "        north = np.max(obs[region][\"lat\"]) + 0.5\n",
        "        width, height = (16,9)\n",
        "        ratioxy = 16./9.\n",
        "        width_ratios = [ratioxy, 1]\n",
        "        lloc = \"center right\"\n",
        "  else:\n",
        "    west = float(custom_southwest.split(\",\")[1])\n",
        "    south = float(custom_southwest.split(\",\")[0])\n",
        "    east = float(custom_northeast.split(\",\")[1])\n",
        "    north = float(custom_northeast.split(\",\")[0])\n",
        "    width, height = (16,7)\n",
        "    width_ratios = [9,7]\n",
        "    lloc = \"lower left\"\n",
        "  #width, height = (16,9)\n",
        "  fig = plt.figure(constrained_layout=True, figsize=(width,height), facecolor=background_color, frameon=True, dpi=150)\n",
        "  if (region_selection == \"CWA\"):\n",
        "    dataframeid = cwa_id\n",
        "  else:\n",
        "    dataframeid = region_selection\n",
        "  #ratioxy = 16./9.\n",
        "  #width_ratios = [ratioxy, 1]\n",
        "  grid = fig.add_gridspec(1,2, hspace=0.2, width_ratios=width_ratios, height_ratios = [1], wspace=0.2)\n",
        "  ax1 = fig.add_subplot(grid[0,0], projection=ccrs.Mercator())\n",
        "  #ax1 = fig.add_subplot(grid[0,0], projection=ccrs.LambertConformal(central_latitude=25, central_longitude=265, standard_parallels=(25,25)))\n",
        "  ax2 = fig.add_subplot(grid[0,1], )\n",
        "  fig.text(0.5, 1.05,f'{dataframeid} {title_dict[element][0]} {compare_element} in NBM v{nbm_version} {title_dict[element][1]} Percentile Space',\\\n",
        "           horizontalalignment='center', verticalalignment='bottom', weight='bold',fontsize=20,color=text_color)\n",
        "  fig.text(0.5, 1.05,f'Valid: {valid_title} | NBM Init: {nbm_init_title} | Points: {points_str}', \\\n",
        "           horizontalalignment='center',verticalalignment='top', fontsize=16,color=text_color)\n",
        "\n",
        "  lats = obs[dataframeid][\"lat\"].values\n",
        "  lons = obs[dataframeid][\"lon\"].values\n",
        "  point_data = obs[dataframeid][compare_var].values\n",
        "  mean = obs[dataframeid][compare_var].mean()\n",
        "  median = obs[dataframeid][compare_var].median()\n",
        "  #mode = obs[dataframeid][compare_var].mode().values[0]\n",
        "  proj = ccrs.PlateCarree()\n",
        "  numcols=(abs(np.amax(point_data) - np.amin(point_data))//10) + 1\n",
        "\n",
        "  ax1.set_anchor('N')\n",
        "  ax1.set_facecolor(background_color)\n",
        "  ax1.set_extent([west, east, south, north], crs=proj)\n",
        "  #ax1.add_feature(cfeature.LAND, edgecolor='none', facecolor='#414143', zorder=-1)\n",
        "  ax1.add_feature(cfeature.OCEAN, edgecolor='none', facecolor=map_water_color, zorder=-2)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '50m', edgecolor='none', facecolor=map_land_color, zorder=-1))\n",
        "  #ax1.add_feature(cfeature.LAKES, edgecolor='none', facecolor='#272727', zorder=0)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'lakes', '10m', edgecolor='none', facecolor=map_water_color, zorder=0))\n",
        "  ax1.add_feature(cfeature.BORDERS, edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=2)\n",
        "  #ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'countries', '50m', edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=2))\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'admin_1_states_provinces_lines', '50m', edgecolor=map_border_color, facecolor='none', linewidth=1, zorder=5))\n",
        "  #ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'admin_1_states_provinces_lines', '50m', edgecolor=map_border, facecolor='none', linewidth=1, zorder=5))\n",
        "  #cx.add_basemap(ax1, source='https://server.arcgisonline.com/ArcGIS/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}', attribution=False)\n",
        "  scatter = ax1.scatter(lons, lats, c= point_data, cmap=cmap, s=45, transform=proj, zorder=2, vmin=0.0, vmax=100.0)\n",
        "  if plot_cities:\n",
        "    plot_towns(ax1, south, north, west, east, population=pop_thresh)\n",
        "  if region_selection in (\"CR\",\"SR\"):\n",
        "    handles, labels = scatter.legend_elements(num=numcols)\n",
        "    legend1 = ax1.legend(flip(handles, 6), flip(labels, 6), ncol=6,loc=lloc, title=f'{compare_element} in NBM Percentile Space', fancybox=True)\n",
        "  else:\n",
        "    legend1 = ax1.legend(*scatter.legend_elements(num=numcols),\n",
        "                      loc=lloc, title=f'{compare_element} \\n Rank', fancybox=True)\n",
        "  plt.setp(legend1.get_title(), multialignment='center', color=text_color)\n",
        "  for text in legend1.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax1.add_artist(legend1)\n",
        "  #ax1.set(aspect='equal', adjustable='box')\n",
        "  if cwa_outline:\n",
        "    try:\n",
        "      if os.path.exists(\"shp/w_22mr22.shp\"):\n",
        "        pass\n",
        "      else:\n",
        "        cwa_url = \"https://www.weather.gov/source/gis/Shapefiles/WSOM/w_22mr22.zip\"\n",
        "        os.mkdir(\"shp\")\n",
        "        urlretrieve(cwa_url, \"shp/nws_cwa_outlines.zip\")\n",
        "        #!unzip shp/nws_cwa_outlines.zip -d shp\n",
        "      with zipfile.ZipFile(\"shp/nws_cwa_outlines.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"shp\")\n",
        "        cwa_feature = ShapelyFeature(Reader(\"shp/w_22mr22.shp\").geometries(),ccrs.PlateCarree(), edgecolor='black', facecolor='none', linewidth=1, linestyle='-', zorder=4)\n",
        "        ax1.add_feature(cwa_feature)\n",
        "    except:\n",
        "      print(\"Aw shucks, no CWA boundaries for you. Sorry bout that.\")\n",
        "  if county_outline:\n",
        "    try:\n",
        "      if os.path.exists(\"shp/c_08mr23.shp\"):\n",
        "        pass\n",
        "      else:\n",
        "        county_url = \"https://www.weather.gov/source/gis/Shapefiles/County/c_08mr23.zip\"\n",
        "        if os.path.exists(\"shp/counties.zip\"):\n",
        "          pass\n",
        "        else:\n",
        "          if os.path.exists(\"shp\"):\n",
        "            pass\n",
        "          else:\n",
        "            os.mkdir(\"shp\")\n",
        "          urlretrieve(county_url, \"shp/counties.zip\")\n",
        "          print(\"   >> Downloaded county zip file\")\n",
        "\n",
        "      with zipfile.ZipFile(\"shp/counties.zip\",'r') as cty_ref:\n",
        "        cty_ref.extractall(\"shp\")\n",
        "        print(\"   >> Extracted county shape files\")\n",
        "        cty_feature = ShapelyFeature(Reader(\"shp/c_08mr23.shp\").geometries(),ccrs.PlateCarree(), edgecolor='grey', facecolor='none', linewidth=0.5, linestyle=':', zorder=3)\n",
        "        ax1.add_feature(cty_feature)\n",
        "    except:\n",
        "      print(\"   >> Cannot plot county boundaries.\")\n",
        "\n",
        "  #if region_selection == \"SR\":\n",
        "  #ax2.set(aspect=1)\n",
        "  ax2.set_anchor('C')\n",
        "  sns.histplot(data=obs[dataframeid], x=compare_var, ax=ax2, kde=True, bins=range(-10,115,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax2.set_xlabel(f'{compare_element} in NBM Percentile Bins', color=text_color, fontsize=12)\n",
        "  ax2.axvline(mean, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax2.axvline(median, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "\n",
        "  ax2.grid(False)\n",
        "  for tick in ax2.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax2.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax2.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend2 = ax2.legend()\n",
        "  for text in legend2.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax2.set(ylabel=None)\n",
        "\n",
        "if custom_area:\n",
        "  figname=f\"{custom_area_name}_map_\"+nbm_version+\"_\"+dataframeid+\"_\"+compare_element+\"_\"+element+\"_\"+nbm_init.strftime('%Y%m%d')+\"_\"+valid_end_datetime.strftime('%Y%m%d')+\".png\"\n",
        "else:\n",
        "  figname=\"map_\"+nbm_version+\"_\"+dataframeid+\"_\"+compare_element+\"_\"+element+\"_\"+nbm_init.strftime('%Y%m%d')+\"_\"+valid_end_datetime.strftime('%Y%m%d')+\".png\"\n",
        "plt.savefig(figname, facecolor=fig.get_facecolor(), bbox_inches='tight', pad_inches=0.2, dpi='figure')\n",
        "print(f'   > Done! Saved plot as {figname}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6. Zip up all images on the left for download\n",
        "\n",
        "!zip -r NBM_Data.zip *.png"
      ],
      "metadata": {
        "cellView": "form",
        "id": "q-HrVgAawoXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7. Remove images\n",
        "!rm *.png\n",
        "!rm *.zip"
      ],
      "metadata": {
        "cellView": "form",
        "id": "f5ak6gwOxJEf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}